# Download and preprocess the COCO dataset.
#
# Instructions:
#   1. Follow the instructions on https://cloud.google.com/tpu/docs/kubernetes-engine-setup
#      to create a Kubernetes Engine cluster. The Job must be running at least
#      on a n1-standard-4 machine.
#   2. Change the environment variable DATA_BUCKET below to the path of the
#      Google Cloud Storage bucket where you want to store the training data.
#   3. Run `kubectl create -f download_and_preprocess_coco_k8s.yaml`.

apiVersion: batch/v1
kind: Job
metadata:
  name: download-and-preprocess-coco
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: download-and-preprocess-coco
        # The official TensorFlow 1.13 TPU model image built from https://github.com/tensorflow/tpu/blob/r1.13/tools/docker/Dockerfile.
        image: gcr.io/tensorflow/tpu-models:r1.13
        command:
          - /bin/bash
          - -c
          - >
            DEBIAN_FRONTEND=noninteractive apt-get update &&
            cd /tensorflow_tpu_models/tools/datasets &&
            bash download_and_preprocess_coco.sh /scratch-dir &&
            gsutil -m cp /scratch-dir/*.tfrecord ${DATA_BUCKET}/coco &&
            gsutil cp /scratch-dir/raw-data/annotations/*.json ${DATA_BUCKET}/coco
        env:
          # [REQUIRED] Must specify the Google Cloud Storage location where the
          # COCO dataset will be stored.
        - name: DATA_BUCKET
          value: "gs://<my-data-bucket>/data/coco"
        volumeMounts:
        - mountPath: /scratch-dir
          name: scratch-volume
      volumes:
      - name: scratch-volume
        persistentVolumeClaim:
          claimName: scratch-disk-coco
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scratch-disk-coco
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
